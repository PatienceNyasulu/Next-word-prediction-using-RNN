# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xO0ftzzjvBLChLvfk1zqRfaF6rBOmhGP
"""



import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical

"""1.Dataset"""

from google.colab import drive
drive.mount('/content/drive')

file = open("/content/drive/MyDrive/shona_dataset.txt","r")
shona_text = file.read()

shona_text

import string
import re

# Convert to lowercase
shona_text = shona_text.lower()

# Remove punctuation marks and special characters
shona_text = re.sub(r"[^\w\s]", "", shona_text)

# Tokenize the text into words
tokens = shona_text.split()

print(tokens)

"""2. Creating a Vocabulary"""

from keras.preprocessing.text import Tokenizer

# Create a tokenizer
tokenizer = Tokenizer()

# Fit the tokenizer on the preprocessed Shona text
tokenizer.fit_on_texts(tokens)

# Define the maximum vocabulary size (e.g., 5000)
vocab_size = 200

# Limit the vocabulary size
tokenizer.num_words = vocab_size

# Get the word index from the tokenizer
word_index = tokenizer.word_index

# Print the word index
print(word_index)

# Tokenize and preprocess
tokenizer = Tokenizer()
tokenizer.fit_on_texts(tokens)
total_words = len(tokenizer.word_index) + 1

"""Genshim Word Embeddings"""

from gensim.models import Word2Vec

# Train word2vec model with gensim
sentences = [sentence.split() for sentence in shona_text.split('.')]
model_gensim = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
model_gensim.save("word2vec.model")

"""RNN Model 1"""

model1 = Sequential()
model1.add(Embedding(total_words, 100, input_length=5))
model1.add(Bidirectional(LSTM(150, return_sequences=True)))
model1.add(Dropout(0.2))
model1.add(LSTM(100))
model1.add(Dense(total_words, activation='softmax'))

model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""RNN Model 2 with Pretrained embeddings"""

# Load the word2vec model
model_gensim = Word2Vec.load("word2vec.model")
embedding_matrix = np.zeros((total_words, 100))
for word, i in tokenizer.word_index.items():
    try:
        embedding_vector = model_gensim.wv[word]
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
    except KeyError:
        # Word not present in gensim model, a zero embedding will be used for this word
        pass


model2 = Sequential()
model2.add(Embedding(total_words, 100, weights=[embedding_matrix], input_length=5, trainable=False))
model2.add(Bidirectional(LSTM(150, return_sequences=True)))
model2.add(Dropout(0.2))
model2.add(LSTM(100))
model2.add(Dense(total_words, activation='softmax'))

model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

"""Training Models"""

import keras
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

input_sequences = []
for line in shona_text.split('.'):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))
X, y = input_sequences[:, :-1], input_sequences[:, -1]
y = keras.utils.to_categorical(y, num_classes=total_words)

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)

# Training Model 1
history1 = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)

# Training Model 2
history2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)

"""Model Evaluation"""

val_loss_model1 = history1.history['val_loss'][-1]
val_loss_model2 = history2.history['val_loss'][-1]

print(f"Validation Loss for Model 1: {val_loss_model1}")
print(f"Validation Loss for Model 2: {val_loss_model2}")

if val_loss_model1 < val_loss_model2:
    best_model = model1
    best_model_name = "best_model1.h5"
else:
    best_model = model2
    best_model_name = "best_model2.h5"

best_model.save(best_model_name)
print(f"Saved the best model as {best_model_name}")

params_model1 = model1.count_params()
params_model2 = model2.count_params()

print(f"Model 1 has {params_model1} parameters.")
print(f"Model 2 has {params_model2} parameters.")

from keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the previously saved model
model = load_model('best_model1.h5')

def predict_next_words(model, tokenizer, text, num_words=1):
    """
    Predict the next set of words using the trained model.

    Args:
    - model (keras.Model): The trained model.
    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.
    - text (str): The input text.
    - num_words (int): The number of words to predict.

    Returns:
    - str: The predicted words.
    """
    for _ in range(num_words):
        # Tokenize and pad the text
        sequence = tokenizer.texts_to_sequences([text])[0]
        sequence = pad_sequences([sequence], maxlen=5, padding='pre')

        # Predict the next word
        predicted_probs = model.predict(sequence, verbose=0)
        predicted = np.argmax(predicted_probs, axis=-1)

        # Convert the predicted word index to a word
        output_word = ""
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break

        # Append the predicted word to the text
        text += " " + output_word

    return ' '.join(text.split(' ')[-num_words:])


# Prompt the user for input
user_input = input("Please type five words in Shona: ")

# Predict the next words
predicted_words = predict_next_words(model, tokenizer, user_input, num_words=3)
print(f"The next words might be: {predicted_words}")

pip install streamlit

import re
import numpy as np
import streamlit as st
from keras.models import load_model
from keras.preprocessing.text import Tokenizer

# Load the tokenizer
tokenizer = Tokenizer()
tokenizer_path = 'tokenizer.pickle'
tokenizer = Tokenizer()
tokenizer.fit_on_texts([''])

# Load the trained model
model_path = 'best_model1.h5'
model = load_model(model_path)

def preprocess_input(text):
    # Convert to lowercase
    text = text.lower()

    # Remove punctuation marks and special characters
    text = re.sub(r"[^\w\s]", "", text)

    # Tokenize the text into words
    tokens = text.split()

    return tokens

def generate_next_words(input_text, num_words):
    # Preprocess the input text
    input_tokens = preprocess_input(input_text)

    # Convert tokens to word indices
    input_sequence = tokenizer.texts_to_sequences([input_tokens])[0]

    # Pad the sequence to a fixed length (if necessary)
    seq_length = len(input_sequence[0])
    max_sequence_length = seq_length# Specify the maximum sequence length used during training
    input_sequence = np.pad(input_sequence, (max_sequence_length - len(input_sequence), 0), 'constant')

    # Generate the next word(s)
    generated_words = input_tokens.copy()

    for _ in range(num_words):
        predicted_word_index = model.predict_classes(input_sequence.reshape(1, -1), verbose=0)
        predicted_word = tokenizer.index_word[predicted_word_index[0]]
        generated_words.append(predicted_word)
        input_sequence = np.roll(input_sequence, -1)
        input_sequence[-1] = predicted_word_index[0]

    return ' '.join(generated_words)

def main():
    st.title('Shona Text Generator')

    # User input
    user_input = st.text_input('Enter five words in Shona:', max_chars=100)

    if user_input:
        num_words_to_generate = 1  # Change this to generate multiple words

        # Generate next word(s)
        generated_words = generate_next_words(user_input, num_words_to_generate)
        st.markdown('**Generated Text:**')
        st.write(generated_words)

if __name__ == '__main__':
    main()