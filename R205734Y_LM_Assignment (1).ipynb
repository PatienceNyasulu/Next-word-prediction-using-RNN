{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuJfuF_6SiHr"
      },
      "outputs": [],
      "source": [
        "#importing all necessary depencies\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Dataset"
      ],
      "metadata": {
        "id": "li4teSkwZ604"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7VO1N0laEpf",
        "outputId": "4eb0f596-3bf2-4181-c189-c008be2675ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/drive/MyDrive/shona_dataset.txt\",\"r\")\n",
        "shona_text = file.read()"
      ],
      "metadata": {
        "id": "IDlh2gLMWWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shona_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "OjulDwZYcWPr",
        "outputId": "caa7c0f0-ced5-4229-a401-76a773f638da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeffSezvo kucherechedza hunhu nekodzero yakayenzana yomunhu wese pasi pose iriyo mviromviro yomutongero uri pachokwadi norunyararo panyika.Sezvo kusatevera nekusvora kodzero dzevanhu zvakamboita kuti kuve noutsinye\\nhwakarwadza pfungwa dzevanhu pasi pose, nokuuya kwenyika inevanhu vachafadzwa norusununguko nokutaura zvavanoda zvakapikirwa sezvinangwa zvinodiwa kuvanhu vese vese.Sezvo chiri chinhu chinokosha, kana munhu asingazomanikidzirwa kuita somukana wokupedzisira, kupandukira hutongi hwakaipa kana hudzvanyiriri, kuti, kodzero dzevanhu dzichengetedzwe nohutongi hwomutemo. Sezvo vanhu vese veMubatanidzwa weNyika, muchisununguko ichi, varatidzazve chitendero chavo mukodzero dzavanhu dzinokosha, uyewo muhunhu nohukoshi homunhu wega wega, munezvekodzero yakayenzana yavanhurume navanhukadzi, zvekare vazvishingisa kukurudzira budiriro mumagariro nokusimudzirwa kohupenyu huri nani murusununguko rwunokosha rwakawedzerwa. Sezvo kunzwisisa pamwechete kodzero idzi norusununguko zvichikosha zvakanyanya kuti chisungo chibudirire.\\nSaka Nokudaro GUNGANO ROMUBATANIDZWA WENYIKA DZEPASI POSE rinozivisa\\nNEZVEKURUDZIRO INO YEKUCHENGETEDZVA KWEKODZERO DZEVANHU PASI POSE sechiratidzo chokubudirira pamwechete kwavanhu vese nemarudzi ese,\\npedzisire munhu wese nenhengo dzese dzemunyika vavekuchengetedza kurudziro ino, vachishingirira nokudzidzisa uyewo kuita mabasa ekudzidzisa nokukurudzira kuremekedzwa kwekodzero dzino nerusununguko namatanho okubudirira munyika kana pasi pose, kuita kuti kodzero idzi dzicherechedzwe zvakakwana nokuchengetedzwa pasi pose nevanhu vemunyika dzinova nhengo uyewo vanhu vanogara mumatunhu ari pasi penyika idzodzo. Chisungo cheKutanga vanhu vese vanoberekwa vakasununguka uyewo vakaenzana pahunhu nekodzero dzavo. Vanhu vese vanechipo chokufunga nekuziva chakaipa nechakanaka saka vanofanira kubatana nomweya wohusahwira.Chisungo cheChipiri munhu wese anofanirwa kuwana kodzero norusununguko samataurirwo azvakaitwa mukurudziro ino, pasina kusarura kupi zvako, kwakaita sekwerudzi rwake, ruvara rwake kana kuti munhurume kana kuti munhukadzi here, mutauro wake, svondo yake, kana kuti mafungiro ake munezvematongerwo enyika kana zvipi zvazvo. Zvekare, hapana kusarura kuchaitwa nokuda kwemamiriro munezvematongerwo enyika, mutongero kana kuti mamiriro enyika kana dunhu raanogara muneipi zvayo nyika pasi pose, ingava nyika yakasununguka, nyika iri kufanotongwa, kana nyika isati yawana kuzvitonga kuzere kana chipi zvacho chipingidzo pakuzvitonga.Chisungo cheChitatu\\nMunhu wese anekodzero yokurarama, kuvanerusununguko nokuzvichengetedza iye\\nomene.Chisungo cheChina hapana munhu achachengetwa muhuranda kana kuitwa semuranda, huranda nekutengeswa kwevanhu neipi zvayo nzira hazvizotenderwi.Chisungo cheChishanu hapana munhu anofanirwa kurwadziswa kana kuitirwa hutsinye, kubatwa semhuka kana kuitwa zvinoderedza, kana upi zvawo murangiro wakadaro. Chisungo cheChitanhatu vanhu vese vakayenzana pamberi pomutemo uye vanekodzero, pasina rusarura, yokuchengetedzwa zvakayenzana nomutemo. Chisungo cheChinomwe munhu wese anekodzero yokuenzanisirwa chichemo chake zvakakwana namatare ari pachokwadi emunyika make apo anenge aitirwa zvinotyora kodzero dzinokosha dzaanopiwa nebumbiro remitemo kana iwo mutemo pachawo. Munhu wese anenzira yekudzivirirwa yakafanana neyavamwe kubva murusarurwo rwose zvarwo runotyora mitemo yekurudziro ino. Chisungo cheChisere munhu wese anekodzero yekumirirwa nematare kana akatyorerwa kodzero dzake dzaainadzo pasi pebumbiro remitemo kana mitemo pachayo. Chisungo cheChipfumbamwe hapana munhu achasungwa, kupfigirwa mutorongo kana kudzingwa munyika make zvisiri pamutemo. Chisungo cheGumi munhu wese anofanirwa kuwana mukana wakaenzana uri pachokwadi wokumira pamberi pedare remhosva rakasununguka, risinadivi rarinorerekera, apo kodzero yake nezvaanotarisirwa kuti aite zvinenge zvichivhenekwa, uyewo kana achinge achipomerwa mhosva ipi zvayo.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gWRrN4OsF3t",
        "outputId": "2c19accf-1fd8-4177-be8e-31f4a1680b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "HbE2GbClrnv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the text\n",
        "def clean_text(shona_text):\n",
        "    # Convert text to lowercase\n",
        "    shona_text = shona_text.lower()\n",
        "\n",
        "    # Remove punctuation and special characters\n",
        "    shona_text = re.sub(r'[^\\w\\s]', '', shona_text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(shona_text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Join the words back into a cleaned text\n",
        "    cleaned_text = ' '.join(words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "cleaned_text = clean_text(shona_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR93kIMbrOkl",
        "outputId": "40c79927-4fff-4f48-e823-d7a9f5b042c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "D0ZAJUcssYNS",
        "outputId": "cddcf99a-2128-4f86-ec48-ade033886417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sezvo kucherechedza hunhu nekodzero yakayenzana yomunhu wese pasi pose iriyo mviromviro yomutongero uri pachokwadi norunyararo panyikasezvo kusatevera nekusvora kodzero dzevanhu zvakamboita kuti kuve noutsinye hwakarwadza pfungwa dzevanhu pasi pose nokuuya kwenyika inevanhu vachafadzwa norusununguko nokutaura zvavanoda zvakapikirwa sezvinangwa zvinodiwa kuvanhu vese vesesezvo chiri chinhu chinokosha kana munhu asingazomanikidzirwa kuita somukana wokupedzisira kupandukira hutongi hwakaipa kana hudzvanyiriri kuti kodzero dzevanhu dzichengetedzwe nohutongi hwomutemo sezvo vanhu vese vemubatanidzwa wenyika muchisununguko ichi varatidzazve chitendero chavo mukodzero dzavanhu dzinokosha uyewo muhunhu nohukoshi homunhu wega wega munezvekodzero yakayenzana yavanhurume navanhukadzi zvekare vazvishingisa kukurudzira budiriro mumagariro nokusimudzirwa kohupenyu huri nani murusununguko rwunokosha rwakawedzerwa sezvo kunzwisisa pamwechete kodzero idzi norusununguko zvichikosha zvakanyanya kuti chisungo chibudirire saka nokudaro gungano romubatanidzwa wenyika dzepasi pose rinozivisa nezvekurudziro ino yekuchengetedzva kwekodzero dzevanhu pasi pose sechiratidzo chokubudirira pamwechete kwavanhu vese nemarudzi ese pedzisire munhu wese nenhengo dzese dzemunyika vavekuchengetedza kurudziro ino vachishingirira nokudzidzisa uyewo kuita mabasa ekudzidzisa nokukurudzira kuremekedzwa kwekodzero dzino nerusununguko namatanho okubudirira munyika kana pasi pose kuita kuti kodzero idzi dzicherechedzwe zvakakwana nokuchengetedzwa pasi pose nevanhu vemunyika dzinova nhengo uyewo vanhu vanogara mumatunhu ari pasi penyika idzodzo chisungo chekutanga vanhu vese vanoberekwa vakasununguka uyewo vakaenzana pahunhu nekodzero dzavo vanhu vese vanechipo chokufunga nekuziva chakaipa nechakanaka saka vanofanira kubatana nomweya wohusahwirachisungo chechipiri munhu wese anofanirwa kuwana kodzero norusununguko samataurirwo azvakaitwa mukurudziro ino pasina kusarura kupi zvako kwakaita sekwerudzi rwake ruvara rwake kana kuti munhurume kana kuti munhukadzi mutauro wake svondo yake kana kuti mafungiro ake munezvematongerwo enyika kana zvipi zvazvo zvekare hapana kusarura kuchaitwa nokuda kwemamiriro munezvematongerwo enyika mutongero kana kuti mamiriro enyika kana dunhu raanogara muneipi zvayo nyika pasi pose ingava nyika yakasununguka nyika iri kufanotongwa kana nyika isati yawana kuzvitonga kuzere kana chipi zvacho chipingidzo pakuzvitongachisungo chechitatu munhu wese anekodzero yokurarama kuvanerusununguko nokuzvichengetedza iye omenechisungo chechina hapana munhu achachengetwa muhuranda kana kuitwa semuranda huranda nekutengeswa kwevanhu neipi zvayo nzira hazvizotenderwichisungo chechishanu hapana munhu anofanirwa kurwadziswa kana kuitirwa hutsinye kubatwa semhuka kana kuitwa zvinoderedza kana upi zvawo murangiro wakadaro chisungo chechitanhatu vanhu vese vakayenzana pamberi pomutemo uye vanekodzero pasina rusarura yokuchengetedzwa zvakayenzana nomutemo chisungo chechinomwe munhu wese anekodzero yokuenzanisirwa chichemo chake zvakakwana namatare ari pachokwadi emunyika make apo anenge aitirwa zvinotyora kodzero dzinokosha dzaanopiwa nebumbiro remitemo kana iwo mutemo pachawo munhu wese anenzira yekudzivirirwa yakafanana neyavamwe kubva murusarurwo rwose zvarwo runotyora mitemo yekurudziro ino chisungo chechisere munhu wese anekodzero yekumirirwa nematare kana akatyorerwa kodzero dzake dzaainadzo pasi pebumbiro remitemo kana mitemo pachayo chisungo chechipfumbamwe hapana munhu achasungwa kupfigirwa mutorongo kana kudzingwa munyika make zvisiri pamutemo chisungo chegumi munhu wese anofanirwa kuwana mukana wakaenzana uri pachokwadi wokumira pamberi pedare remhosva rakasununguka risinadivi rarinorerekera apo kodzero yake nezvaanotarisirwa kuti aite zvinenge zvichivhenekwa uyewo kana achinge achipomerwa mhosva ipi zvayo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(cleaned_text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Z0wCmBljQe7-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf35281-d5f0-4641-ebe8-c47ef9240491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sezvo', 'kucherechedza', 'hunhu', 'nekodzero', 'yakayenzana', 'yomunhu', 'wese', 'pasi', 'pose', 'iriyo', 'mviromviro', 'yomutongero', 'uri', 'pachokwadi', 'norunyararo', 'panyikasezvo', 'kusatevera', 'nekusvora', 'kodzero', 'dzevanhu', 'zvakamboita', 'kuti', 'kuve', 'noutsinye', 'hwakarwadza', 'pfungwa', 'dzevanhu', 'pasi', 'pose', 'nokuuya', 'kwenyika', 'inevanhu', 'vachafadzwa', 'norusununguko', 'nokutaura', 'zvavanoda', 'zvakapikirwa', 'sezvinangwa', 'zvinodiwa', 'kuvanhu', 'vese', 'vesesezvo', 'chiri', 'chinhu', 'chinokosha', 'kana', 'munhu', 'asingazomanikidzirwa', 'kuita', 'somukana', 'wokupedzisira', 'kupandukira', 'hutongi', 'hwakaipa', 'kana', 'hudzvanyiriri', 'kuti', 'kodzero', 'dzevanhu', 'dzichengetedzwe', 'nohutongi', 'hwomutemo', 'sezvo', 'vanhu', 'vese', 'vemubatanidzwa', 'wenyika', 'muchisununguko', 'ichi', 'varatidzazve', 'chitendero', 'chavo', 'mukodzero', 'dzavanhu', 'dzinokosha', 'uyewo', 'muhunhu', 'nohukoshi', 'homunhu', 'wega', 'wega', 'munezvekodzero', 'yakayenzana', 'yavanhurume', 'navanhukadzi', 'zvekare', 'vazvishingisa', 'kukurudzira', 'budiriro', 'mumagariro', 'nokusimudzirwa', 'kohupenyu', 'huri', 'nani', 'murusununguko', 'rwunokosha', 'rwakawedzerwa', 'sezvo', 'kunzwisisa', 'pamwechete', 'kodzero', 'idzi', 'norusununguko', 'zvichikosha', 'zvakanyanya', 'kuti', 'chisungo', 'chibudirire', 'saka', 'nokudaro', 'gungano', 'romubatanidzwa', 'wenyika', 'dzepasi', 'pose', 'rinozivisa', 'nezvekurudziro', 'ino', 'yekuchengetedzva', 'kwekodzero', 'dzevanhu', 'pasi', 'pose', 'sechiratidzo', 'chokubudirira', 'pamwechete', 'kwavanhu', 'vese', 'nemarudzi', 'ese', 'pedzisire', 'munhu', 'wese', 'nenhengo', 'dzese', 'dzemunyika', 'vavekuchengetedza', 'kurudziro', 'ino', 'vachishingirira', 'nokudzidzisa', 'uyewo', 'kuita', 'mabasa', 'ekudzidzisa', 'nokukurudzira', 'kuremekedzwa', 'kwekodzero', 'dzino', 'nerusununguko', 'namatanho', 'okubudirira', 'munyika', 'kana', 'pasi', 'pose', 'kuita', 'kuti', 'kodzero', 'idzi', 'dzicherechedzwe', 'zvakakwana', 'nokuchengetedzwa', 'pasi', 'pose', 'nevanhu', 'vemunyika', 'dzinova', 'nhengo', 'uyewo', 'vanhu', 'vanogara', 'mumatunhu', 'ari', 'pasi', 'penyika', 'idzodzo', 'chisungo', 'chekutanga', 'vanhu', 'vese', 'vanoberekwa', 'vakasununguka', 'uyewo', 'vakaenzana', 'pahunhu', 'nekodzero', 'dzavo', 'vanhu', 'vese', 'vanechipo', 'chokufunga', 'nekuziva', 'chakaipa', 'nechakanaka', 'saka', 'vanofanira', 'kubatana', 'nomweya', 'wohusahwirachisungo', 'chechipiri', 'munhu', 'wese', 'anofanirwa', 'kuwana', 'kodzero', 'norusununguko', 'samataurirwo', 'azvakaitwa', 'mukurudziro', 'ino', 'pasina', 'kusarura', 'kupi', 'zvako', 'kwakaita', 'sekwerudzi', 'rwake', 'ruvara', 'rwake', 'kana', 'kuti', 'munhurume', 'kana', 'kuti', 'munhukadzi', 'mutauro', 'wake', 'svondo', 'yake', 'kana', 'kuti', 'mafungiro', 'ake', 'munezvematongerwo', 'enyika', 'kana', 'zvipi', 'zvazvo', 'zvekare', 'hapana', 'kusarura', 'kuchaitwa', 'nokuda', 'kwemamiriro', 'munezvematongerwo', 'enyika', 'mutongero', 'kana', 'kuti', 'mamiriro', 'enyika', 'kana', 'dunhu', 'raanogara', 'muneipi', 'zvayo', 'nyika', 'pasi', 'pose', 'ingava', 'nyika', 'yakasununguka', 'nyika', 'iri', 'kufanotongwa', 'kana', 'nyika', 'isati', 'yawana', 'kuzvitonga', 'kuzere', 'kana', 'chipi', 'zvacho', 'chipingidzo', 'pakuzvitongachisungo', 'chechitatu', 'munhu', 'wese', 'anekodzero', 'yokurarama', 'kuvanerusununguko', 'nokuzvichengetedza', 'iye', 'omenechisungo', 'chechina', 'hapana', 'munhu', 'achachengetwa', 'muhuranda', 'kana', 'kuitwa', 'semuranda', 'huranda', 'nekutengeswa', 'kwevanhu', 'neipi', 'zvayo', 'nzira', 'hazvizotenderwichisungo', 'chechishanu', 'hapana', 'munhu', 'anofanirwa', 'kurwadziswa', 'kana', 'kuitirwa', 'hutsinye', 'kubatwa', 'semhuka', 'kana', 'kuitwa', 'zvinoderedza', 'kana', 'upi', 'zvawo', 'murangiro', 'wakadaro', 'chisungo', 'chechitanhatu', 'vanhu', 'vese', 'vakayenzana', 'pamberi', 'pomutemo', 'uye', 'vanekodzero', 'pasina', 'rusarura', 'yokuchengetedzwa', 'zvakayenzana', 'nomutemo', 'chisungo', 'chechinomwe', 'munhu', 'wese', 'anekodzero', 'yokuenzanisirwa', 'chichemo', 'chake', 'zvakakwana', 'namatare', 'ari', 'pachokwadi', 'emunyika', 'make', 'apo', 'anenge', 'aitirwa', 'zvinotyora', 'kodzero', 'dzinokosha', 'dzaanopiwa', 'nebumbiro', 'remitemo', 'kana', 'iwo', 'mutemo', 'pachawo', 'munhu', 'wese', 'anenzira', 'yekudzivirirwa', 'yakafanana', 'neyavamwe', 'kubva', 'murusarurwo', 'rwose', 'zvarwo', 'runotyora', 'mitemo', 'yekurudziro', 'ino', 'chisungo', 'chechisere', 'munhu', 'wese', 'anekodzero', 'yekumirirwa', 'nematare', 'kana', 'akatyorerwa', 'kodzero', 'dzake', 'dzaainadzo', 'pasi', 'pebumbiro', 'remitemo', 'kana', 'mitemo', 'pachayo', 'chisungo', 'chechipfumbamwe', 'hapana', 'munhu', 'achasungwa', 'kupfigirwa', 'mutorongo', 'kana', 'kudzingwa', 'munyika', 'make', 'zvisiri', 'pamutemo', 'chisungo', 'chegumi', 'munhu', 'wese', 'anofanirwa', 'kuwana', 'mukana', 'wakaenzana', 'uri', 'pachokwadi', 'wokumira', 'pamberi', 'pedare', 'remhosva', 'rakasununguka', 'risinadivi', 'rarinorerekera', 'apo', 'kodzero', 'yake', 'nezvaanotarisirwa', 'kuti', 'aite', 'zvinenge', 'zvichivhenekwa', 'uyewo', 'kana', 'achinge', 'achipomerwa', 'mhosva', 'ipi', 'zvayo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#saving the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "  pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "v0vsEoZAzGqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Creating a Vocabulary"
      ],
      "metadata": {
        "id": "Al6XXqoFfpSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using a keras tokenizer to build an optimal vocabulary\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([cleaned_text])\n",
        "max_vocab_size= 10000  # the vocabulary size\n",
        "word_index = tokenizer.word_index\n",
        "vocabulary_size = min(max_vocab_size, len(word_index))\n"
      ],
      "metadata": {
        "id": "VSBQi2nOQDjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_word_index = {}\n",
        "for word, index in word_index.items():\n",
        "    if index <= vocabulary_size:\n",
        "        reduced_word_index[word] = index\n",
        "    else:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "tokenizer.word_index = reduced_word_index\n",
        "tokenizer.word_index[tokenizer.oov_token] = vocabulary_size + 1\n",
        "tokenizer.num_words = vocabulary_size + 1\n",
        "vocabulary_size = len(word_index)\n",
        "print(\"Vocabulary size:\", vocabulary_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s556lp_fPGKP",
        "outputId": "1d8b19ec-e12a-4852-ead6-52955fcbfbc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Genshim Word Embeddings"
      ],
      "metadata": {
        "id": "2qpAS0QlhmBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train word2vec model with gensim\n",
        "sentences = [sentence.split() for sentence in cleaned_text.split('.')]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec_shona_embeddings.model\")"
      ],
      "metadata": {
        "id": "19lswiSxhTVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Model 1"
      ],
      "metadata": {
        "id": "VktIKb1dhn4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential()\n",
        "model1.add(Embedding(tokenizer.num_words , 100, input_length=5))\n",
        "model1.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model1.add(Dropout(0.2))\n",
        "model1.add(LSTM(100))\n",
        "model1.add(Dense(tokenizer.num_words , activation='softmax'))\n",
        "\n",
        "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#save the trained model\n",
        "model1.save('model1.h1')\n",
        "#print the summary of the model\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "IK3VoWr2hqvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6308c98-eb26-4c0d-8a53-16f835b32160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 5, 100)            30100     \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 5, 300)            301200    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 301)               30401     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 522101 (1.99 MB)\n",
            "Trainable params: 522101 (1.99 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNN Model 2 with Pretrained embeddings"
      ],
      "metadata": {
        "id": "6awS2A-ziOym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the word2vec model\n",
        "model_gensim = Word2Vec.load(\"word2vec_shona_embeddings.model\")\n",
        "embedding_matrix = np.zeros((tokenizer.num_words , 100))\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(tokenizer.num_words , 100, weights=[embedding_matrix], input_length=5, trainable=False))\n",
        "model2.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
        "model2.add(Dropout(0.2))\n",
        "model2.add(LSTM(100))\n",
        "model2.add(Dense(tokenizer.num_words , activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#save the trained model\n",
        "model2.save('model2.h1')\n",
        "#print summary of the model\n",
        "model2.summary()"
      ],
      "metadata": {
        "id": "u19B9BzvhfNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a4bdac5-3304-4ce1-f60e-a26c7e0185d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 5, 100)            30100     \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 5, 300)            301200    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 300)            0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               160400    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 301)               30401     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 522101 (1.99 MB)\n",
            "Trainable params: 492001 (1.88 MB)\n",
            "Non-trainable params: 30100 (117.58 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('model1.h1')\n",
        "model = load_model('model2.h1')\n"
      ],
      "metadata": {
        "id": "X_v5GMLpOnZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Models"
      ],
      "metadata": {
        "id": "65dXanQ1jAND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Q-2uFCH9jIf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "for line in cleaned_text.split('.'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=6, padding='pre'))\n",
        "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y = keras.utils.to_categorical(y, num_classes=tokenizer.num_words)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "2tvL0ML6i_26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)"
      ],
      "metadata": {
        "id": "aOCnGJhB5uar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RNN Model 1\n",
        "history1 = model1.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG2hl9oIjPMF",
        "outputId": "e9a1ae40-120b-4e71-8263-7ea00eb3aa01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 12s 356ms/step - loss: 5.7072 - accuracy: 0.0077 - val_loss: 5.7076 - val_accuracy: 0.0114\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 1s 73ms/step - loss: 5.6928 - accuracy: 0.0460 - val_loss: 5.7097 - val_accuracy: 0.0343\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 1s 69ms/step - loss: 5.6688 - accuracy: 0.0460 - val_loss: 5.7202 - val_accuracy: 0.0629\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 1s 62ms/step - loss: 5.5697 - accuracy: 0.0383 - val_loss: 5.9125 - val_accuracy: 0.0629\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 64ms/step - loss: 5.3149 - accuracy: 0.0345 - val_loss: 6.7301 - val_accuracy: 0.0629\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 1s 100ms/step - loss: 5.1815 - accuracy: 0.0345 - val_loss: 6.5240 - val_accuracy: 0.0629\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 1s 148ms/step - loss: 5.0837 - accuracy: 0.0383 - val_loss: 6.8395 - val_accuracy: 0.0686\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 1s 106ms/step - loss: 4.9708 - accuracy: 0.0421 - val_loss: 7.1468 - val_accuracy: 0.0514\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 1s 125ms/step - loss: 4.8257 - accuracy: 0.0613 - val_loss: 7.3190 - val_accuracy: 0.0514\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 2s 178ms/step - loss: 4.6696 - accuracy: 0.0728 - val_loss: 7.4089 - val_accuracy: 0.0171\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 4.4887 - accuracy: 0.0651 - val_loss: 7.6469 - val_accuracy: 0.0400\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 1s 57ms/step - loss: 4.2498 - accuracy: 0.1073 - val_loss: 7.6139 - val_accuracy: 0.0286\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 1s 65ms/step - loss: 4.0058 - accuracy: 0.1571 - val_loss: 7.6720 - val_accuracy: 0.0343\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 1s 63ms/step - loss: 3.7488 - accuracy: 0.1801 - val_loss: 7.8848 - val_accuracy: 0.0171\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 1s 85ms/step - loss: 3.5456 - accuracy: 0.2452 - val_loss: 7.8651 - val_accuracy: 0.0343\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 1s 105ms/step - loss: 3.3065 - accuracy: 0.3257 - val_loss: 7.9057 - val_accuracy: 0.0171\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 3.0950 - accuracy: 0.4023 - val_loss: 7.7655 - val_accuracy: 0.0114\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 2.8930 - accuracy: 0.4483 - val_loss: 8.0578 - val_accuracy: 0.0171\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 2.7001 - accuracy: 0.5479 - val_loss: 7.9650 - val_accuracy: 0.0114\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 2.5284 - accuracy: 0.5670 - val_loss: 8.0418 - val_accuracy: 0.0229\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 2.3635 - accuracy: 0.6245 - val_loss: 8.1465 - val_accuracy: 0.0057\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 1s 106ms/step - loss: 2.2153 - accuracy: 0.6475 - val_loss: 8.1833 - val_accuracy: 0.0171\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 2.0858 - accuracy: 0.6628 - val_loss: 8.2564 - val_accuracy: 0.0057\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 1.9389 - accuracy: 0.7203 - val_loss: 8.2266 - val_accuracy: 0.0114\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 1.8261 - accuracy: 0.7663 - val_loss: 8.3219 - val_accuracy: 0.0114\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 1.7085 - accuracy: 0.7663 - val_loss: 8.3588 - val_accuracy: 0.0114\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 54ms/step - loss: 1.6066 - accuracy: 0.7969 - val_loss: 8.3587 - val_accuracy: 0.0114\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 1.4864 - accuracy: 0.8391 - val_loss: 8.4526 - val_accuracy: 0.0114\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 1s 97ms/step - loss: 1.4088 - accuracy: 0.8506 - val_loss: 8.4846 - val_accuracy: 0.0114\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 1.3348 - accuracy: 0.8582 - val_loss: 8.5445 - val_accuracy: 0.0114\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 1s 88ms/step - loss: 1.2717 - accuracy: 0.8889 - val_loss: 8.5310 - val_accuracy: 0.0171\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 1.1817 - accuracy: 0.9080 - val_loss: 8.6158 - val_accuracy: 0.0114\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 1s 85ms/step - loss: 1.1318 - accuracy: 0.9234 - val_loss: 8.6154 - val_accuracy: 0.0114\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 1s 57ms/step - loss: 1.0629 - accuracy: 0.9234 - val_loss: 8.7183 - val_accuracy: 0.0057\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.9930 - accuracy: 0.9425 - val_loss: 8.7146 - val_accuracy: 0.0114\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 0.9463 - accuracy: 0.9387 - val_loss: 8.7663 - val_accuracy: 0.0057\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.8879 - accuracy: 0.9464 - val_loss: 8.8413 - val_accuracy: 0.0057\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.8363 - accuracy: 0.9579 - val_loss: 8.8124 - val_accuracy: 0.0057\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 54ms/step - loss: 0.7906 - accuracy: 0.9502 - val_loss: 8.8679 - val_accuracy: 0.0057\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.7763 - accuracy: 0.9425 - val_loss: 8.9423 - val_accuracy: 0.0114\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.7187 - accuracy: 0.9693 - val_loss: 8.9764 - val_accuracy: 0.0057\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.6742 - accuracy: 0.9847 - val_loss: 9.0360 - val_accuracy: 0.0114\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.6279 - accuracy: 0.9808 - val_loss: 9.0455 - val_accuracy: 0.0057\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.5901 - accuracy: 0.9808 - val_loss: 9.1212 - val_accuracy: 0.0057\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.5572 - accuracy: 0.9808 - val_loss: 9.1233 - val_accuracy: 0.0057\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.5255 - accuracy: 0.9885 - val_loss: 9.1743 - val_accuracy: 0.0057\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 0.4983 - accuracy: 1.0000 - val_loss: 9.1776 - val_accuracy: 0.0057\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 57ms/step - loss: 0.4719 - accuracy: 0.9847 - val_loss: 9.2380 - val_accuracy: 0.0057\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 1s 107ms/step - loss: 0.4521 - accuracy: 0.9923 - val_loss: 9.2299 - val_accuracy: 0.0057\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 1s 109ms/step - loss: 0.4320 - accuracy: 0.9923 - val_loss: 9.3572 - val_accuracy: 0.0114\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 0.4174 - accuracy: 0.9962 - val_loss: 9.3478 - val_accuracy: 0.0057\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.3946 - accuracy: 0.9962 - val_loss: 9.3288 - val_accuracy: 0.0171\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.3948 - accuracy: 0.9923 - val_loss: 9.3345 - val_accuracy: 0.0114\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 48ms/step - loss: 0.3699 - accuracy: 1.0000 - val_loss: 9.3709 - val_accuracy: 0.0114\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 1s 83ms/step - loss: 0.3492 - accuracy: 1.0000 - val_loss: 9.4323 - val_accuracy: 0.0171\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 1s 106ms/step - loss: 0.3325 - accuracy: 1.0000 - val_loss: 9.4146 - val_accuracy: 0.0114\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 1s 124ms/step - loss: 0.3163 - accuracy: 1.0000 - val_loss: 9.4192 - val_accuracy: 0.0114\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 1s 125ms/step - loss: 0.3052 - accuracy: 1.0000 - val_loss: 9.4787 - val_accuracy: 0.0114\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 1s 89ms/step - loss: 0.2903 - accuracy: 1.0000 - val_loss: 9.5155 - val_accuracy: 0.0171\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.2783 - accuracy: 1.0000 - val_loss: 9.4991 - val_accuracy: 0.0114\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2641 - accuracy: 1.0000 - val_loss: 9.5645 - val_accuracy: 0.0171\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 0.2545 - accuracy: 1.0000 - val_loss: 9.6003 - val_accuracy: 0.0114\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 1s 63ms/step - loss: 0.2470 - accuracy: 0.9962 - val_loss: 9.6071 - val_accuracy: 0.0114\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2351 - accuracy: 1.0000 - val_loss: 9.5888 - val_accuracy: 0.0114\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.2251 - accuracy: 1.0000 - val_loss: 9.6317 - val_accuracy: 0.0114\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2190 - accuracy: 1.0000 - val_loss: 9.6245 - val_accuracy: 0.0114\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.2099 - accuracy: 1.0000 - val_loss: 9.6472 - val_accuracy: 0.0114\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.2040 - accuracy: 1.0000 - val_loss: 9.6977 - val_accuracy: 0.0114\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.1972 - accuracy: 1.0000 - val_loss: 9.6618 - val_accuracy: 0.0114\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1892 - accuracy: 1.0000 - val_loss: 9.7000 - val_accuracy: 0.0114\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 57ms/step - loss: 0.1830 - accuracy: 1.0000 - val_loss: 9.7608 - val_accuracy: 0.0114\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 1s 60ms/step - loss: 0.1767 - accuracy: 1.0000 - val_loss: 9.7602 - val_accuracy: 0.0057\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 52ms/step - loss: 0.1718 - accuracy: 1.0000 - val_loss: 9.7503 - val_accuracy: 0.0114\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1687 - accuracy: 1.0000 - val_loss: 9.7668 - val_accuracy: 0.0171\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.1632 - accuracy: 1.0000 - val_loss: 9.7703 - val_accuracy: 0.0171\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 0.1593 - accuracy: 1.0000 - val_loss: 9.7890 - val_accuracy: 0.0114\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1585 - accuracy: 1.0000 - val_loss: 9.7880 - val_accuracy: 0.0114\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1549 - accuracy: 1.0000 - val_loss: 9.8525 - val_accuracy: 0.0114\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 0.1476 - accuracy: 1.0000 - val_loss: 9.8658 - val_accuracy: 0.0171\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 56ms/step - loss: 0.1400 - accuracy: 1.0000 - val_loss: 9.8375 - val_accuracy: 0.0171\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.1380 - accuracy: 1.0000 - val_loss: 9.8625 - val_accuracy: 0.0171\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.1339 - accuracy: 1.0000 - val_loss: 9.9023 - val_accuracy: 0.0171\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 1s 72ms/step - loss: 0.1298 - accuracy: 1.0000 - val_loss: 9.8686 - val_accuracy: 0.0171\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 1s 87ms/step - loss: 0.1266 - accuracy: 1.0000 - val_loss: 9.8829 - val_accuracy: 0.0171\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 1s 90ms/step - loss: 0.1213 - accuracy: 1.0000 - val_loss: 9.9364 - val_accuracy: 0.0229\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 1s 108ms/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 9.9508 - val_accuracy: 0.0229\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 1s 87ms/step - loss: 0.1142 - accuracy: 1.0000 - val_loss: 9.9438 - val_accuracy: 0.0229\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 0.1102 - accuracy: 1.0000 - val_loss: 9.9547 - val_accuracy: 0.0171\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 1s 63ms/step - loss: 0.1075 - accuracy: 1.0000 - val_loss: 9.9771 - val_accuracy: 0.0171\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 1s 61ms/step - loss: 0.1045 - accuracy: 1.0000 - val_loss: 9.9855 - val_accuracy: 0.0229\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 52ms/step - loss: 0.1019 - accuracy: 1.0000 - val_loss: 9.9780 - val_accuracy: 0.0229\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.1007 - accuracy: 1.0000 - val_loss: 9.9866 - val_accuracy: 0.0171\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 0.0974 - accuracy: 1.0000 - val_loss: 10.0167 - val_accuracy: 0.0171\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 1s 59ms/step - loss: 0.0948 - accuracy: 1.0000 - val_loss: 10.0122 - val_accuracy: 0.0229\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 55ms/step - loss: 0.0925 - accuracy: 1.0000 - val_loss: 10.0334 - val_accuracy: 0.0229\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 1s 66ms/step - loss: 0.0904 - accuracy: 1.0000 - val_loss: 10.0662 - val_accuracy: 0.0171\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 53ms/step - loss: 0.0887 - accuracy: 1.0000 - val_loss: 10.0647 - val_accuracy: 0.0171\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 0.0866 - accuracy: 1.0000 - val_loss: 10.0707 - val_accuracy: 0.0114\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.0846 - accuracy: 1.0000 - val_loss: 10.0895 - val_accuracy: 0.0114\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 0.0825 - accuracy: 1.0000 - val_loss: 10.1281 - val_accuracy: 0.0171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training RNN Model 2 with pretrained embeddings\n",
        "history2 = model2.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OXMELwPjSmv",
        "outputId": "2e8fbae7-9d54-4aae-8fd4-ef91a7ab21c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 7s 196ms/step - loss: 5.7073 - accuracy: 0.0153 - val_loss: 5.7072 - val_accuracy: 0.0629\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 56ms/step - loss: 5.6972 - accuracy: 0.0345 - val_loss: 5.7084 - val_accuracy: 0.0629\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 1s 78ms/step - loss: 5.6689 - accuracy: 0.0345 - val_loss: 5.7260 - val_accuracy: 0.0629\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 1s 83ms/step - loss: 5.5249 - accuracy: 0.0345 - val_loss: 6.1004 - val_accuracy: 0.0629\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 1s 87ms/step - loss: 5.3392 - accuracy: 0.0345 - val_loss: 6.6919 - val_accuracy: 0.0629\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 1s 96ms/step - loss: 5.2675 - accuracy: 0.0345 - val_loss: 6.8173 - val_accuracy: 0.0629\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 1s 79ms/step - loss: 5.2310 - accuracy: 0.0345 - val_loss: 7.0198 - val_accuracy: 0.0629\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 1s 82ms/step - loss: 5.2151 - accuracy: 0.0345 - val_loss: 7.1698 - val_accuracy: 0.0629\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.2084 - accuracy: 0.0345 - val_loss: 7.2883 - val_accuracy: 0.0629\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 1s 61ms/step - loss: 5.2026 - accuracy: 0.0345 - val_loss: 7.3262 - val_accuracy: 0.0629\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 48ms/step - loss: 5.1992 - accuracy: 0.0345 - val_loss: 7.4513 - val_accuracy: 0.0629\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 46ms/step - loss: 5.2007 - accuracy: 0.0345 - val_loss: 7.4470 - val_accuracy: 0.0629\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1959 - accuracy: 0.0345 - val_loss: 7.5589 - val_accuracy: 0.0629\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1954 - accuracy: 0.0345 - val_loss: 7.6315 - val_accuracy: 0.0629\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1955 - accuracy: 0.0345 - val_loss: 7.7166 - val_accuracy: 0.0629\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1942 - accuracy: 0.0345 - val_loss: 7.7484 - val_accuracy: 0.0629\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1920 - accuracy: 0.0345 - val_loss: 7.7653 - val_accuracy: 0.0629\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1924 - accuracy: 0.0345 - val_loss: 7.7237 - val_accuracy: 0.0629\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1915 - accuracy: 0.0345 - val_loss: 7.8250 - val_accuracy: 0.0629\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1902 - accuracy: 0.0345 - val_loss: 7.7452 - val_accuracy: 0.0629\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1881 - accuracy: 0.0345 - val_loss: 7.8807 - val_accuracy: 0.0629\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1898 - accuracy: 0.0345 - val_loss: 7.9413 - val_accuracy: 0.0629\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 5.1896 - accuracy: 0.0345 - val_loss: 7.9462 - val_accuracy: 0.0629\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 1s 59ms/step - loss: 5.1883 - accuracy: 0.0345 - val_loss: 8.0681 - val_accuracy: 0.0629\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 57ms/step - loss: 5.1886 - accuracy: 0.0345 - val_loss: 8.0773 - val_accuracy: 0.0629\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1891 - accuracy: 0.0345 - val_loss: 8.0224 - val_accuracy: 0.0629\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 5.1859 - accuracy: 0.0345 - val_loss: 7.9294 - val_accuracy: 0.0629\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1865 - accuracy: 0.0345 - val_loss: 7.8857 - val_accuracy: 0.0629\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1866 - accuracy: 0.0345 - val_loss: 8.2135 - val_accuracy: 0.0629\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1878 - accuracy: 0.0345 - val_loss: 8.2164 - val_accuracy: 0.0629\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1857 - accuracy: 0.0345 - val_loss: 8.0565 - val_accuracy: 0.0629\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1853 - accuracy: 0.0345 - val_loss: 7.9584 - val_accuracy: 0.0629\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1844 - accuracy: 0.0345 - val_loss: 8.0444 - val_accuracy: 0.0629\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 1s 85ms/step - loss: 5.1856 - accuracy: 0.0345 - val_loss: 8.1381 - val_accuracy: 0.0629\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 5.1846 - accuracy: 0.0345 - val_loss: 8.2062 - val_accuracy: 0.0629\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 1s 86ms/step - loss: 5.1870 - accuracy: 0.0345 - val_loss: 8.1388 - val_accuracy: 0.0629\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 1s 87ms/step - loss: 5.1895 - accuracy: 0.0345 - val_loss: 8.3831 - val_accuracy: 0.0629\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 1s 88ms/step - loss: 5.1854 - accuracy: 0.0345 - val_loss: 8.2480 - val_accuracy: 0.0629\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 1s 80ms/step - loss: 5.1840 - accuracy: 0.0345 - val_loss: 8.3042 - val_accuracy: 0.0629\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 53ms/step - loss: 5.1832 - accuracy: 0.0345 - val_loss: 8.2156 - val_accuracy: 0.0629\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1823 - accuracy: 0.0345 - val_loss: 8.0277 - val_accuracy: 0.0629\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1823 - accuracy: 0.0345 - val_loss: 8.1007 - val_accuracy: 0.0629\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 5.1827 - accuracy: 0.0345 - val_loss: 8.1996 - val_accuracy: 0.0629\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1841 - accuracy: 0.0345 - val_loss: 8.1678 - val_accuracy: 0.0629\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 5.1844 - accuracy: 0.0345 - val_loss: 8.1553 - val_accuracy: 0.0629\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 1s 59ms/step - loss: 5.1835 - accuracy: 0.0345 - val_loss: 8.3531 - val_accuracy: 0.0629\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 48ms/step - loss: 5.1835 - accuracy: 0.0345 - val_loss: 8.4704 - val_accuracy: 0.0629\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1856 - accuracy: 0.0345 - val_loss: 8.4435 - val_accuracy: 0.0629\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1832 - accuracy: 0.0345 - val_loss: 8.3575 - val_accuracy: 0.0629\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1820 - accuracy: 0.0345 - val_loss: 8.2850 - val_accuracy: 0.0629\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1828 - accuracy: 0.0345 - val_loss: 8.3419 - val_accuracy: 0.0629\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1813 - accuracy: 0.0345 - val_loss: 8.2462 - val_accuracy: 0.0629\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 54ms/step - loss: 5.1855 - accuracy: 0.0345 - val_loss: 8.1082 - val_accuracy: 0.0629\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1791 - accuracy: 0.0345 - val_loss: 8.6313 - val_accuracy: 0.0629\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 1s 94ms/step - loss: 5.1872 - accuracy: 0.0345 - val_loss: 8.8431 - val_accuracy: 0.0629\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1855 - accuracy: 0.0345 - val_loss: 8.6287 - val_accuracy: 0.0629\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1823 - accuracy: 0.0345 - val_loss: 8.3757 - val_accuracy: 0.0629\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1810 - accuracy: 0.0345 - val_loss: 8.3536 - val_accuracy: 0.0629\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1803 - accuracy: 0.0345 - val_loss: 8.4956 - val_accuracy: 0.0629\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1853 - accuracy: 0.0345 - val_loss: 8.6729 - val_accuracy: 0.0629\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1847 - accuracy: 0.0345 - val_loss: 8.6219 - val_accuracy: 0.0629\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 47ms/step - loss: 5.1826 - accuracy: 0.0345 - val_loss: 8.5100 - val_accuracy: 0.0629\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 52ms/step - loss: 5.1815 - accuracy: 0.0345 - val_loss: 8.4942 - val_accuracy: 0.0629\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 1s 95ms/step - loss: 5.1798 - accuracy: 0.0345 - val_loss: 8.4515 - val_accuracy: 0.0629\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 5.1807 - accuracy: 0.0345 - val_loss: 8.3917 - val_accuracy: 0.0629\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 1s 87ms/step - loss: 5.1809 - accuracy: 0.0345 - val_loss: 8.3382 - val_accuracy: 0.0629\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 1s 84ms/step - loss: 5.1804 - accuracy: 0.0345 - val_loss: 8.3930 - val_accuracy: 0.0629\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 1s 89ms/step - loss: 5.1809 - accuracy: 0.0345 - val_loss: 8.4330 - val_accuracy: 0.0629\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 1s 76ms/step - loss: 5.1809 - accuracy: 0.0345 - val_loss: 8.3997 - val_accuracy: 0.0629\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1791 - accuracy: 0.0345 - val_loss: 8.4394 - val_accuracy: 0.0629\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 51ms/step - loss: 5.1796 - accuracy: 0.0345 - val_loss: 8.4215 - val_accuracy: 0.0629\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 1s 62ms/step - loss: 5.1793 - accuracy: 0.0345 - val_loss: 8.3379 - val_accuracy: 0.0629\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 5.1810 - accuracy: 0.0345 - val_loss: 8.5252 - val_accuracy: 0.0629\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1799 - accuracy: 0.0345 - val_loss: 8.5722 - val_accuracy: 0.0629\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1801 - accuracy: 0.0345 - val_loss: 8.4406 - val_accuracy: 0.0629\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1798 - accuracy: 0.0345 - val_loss: 8.4052 - val_accuracy: 0.0629\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 56ms/step - loss: 5.1780 - accuracy: 0.0345 - val_loss: 8.5258 - val_accuracy: 0.0629\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1786 - accuracy: 0.0345 - val_loss: 8.7409 - val_accuracy: 0.0629\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1812 - accuracy: 0.0345 - val_loss: 8.8352 - val_accuracy: 0.0629\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1797 - accuracy: 0.0345 - val_loss: 8.7326 - val_accuracy: 0.0629\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1790 - accuracy: 0.0345 - val_loss: 8.5652 - val_accuracy: 0.0629\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 45ms/step - loss: 5.1791 - accuracy: 0.0345 - val_loss: 8.4640 - val_accuracy: 0.0629\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1778 - accuracy: 0.0345 - val_loss: 8.5151 - val_accuracy: 0.0629\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 50ms/step - loss: 5.1800 - accuracy: 0.0345 - val_loss: 8.6936 - val_accuracy: 0.0629\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 1s 60ms/step - loss: 5.1768 - accuracy: 0.0345 - val_loss: 8.5816 - val_accuracy: 0.0629\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 54ms/step - loss: 5.1779 - accuracy: 0.0345 - val_loss: 8.3155 - val_accuracy: 0.0629\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 40ms/step - loss: 5.1805 - accuracy: 0.0345 - val_loss: 8.2507 - val_accuracy: 0.0629\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1793 - accuracy: 0.0345 - val_loss: 8.4336 - val_accuracy: 0.0629\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 42ms/step - loss: 5.1784 - accuracy: 0.0345 - val_loss: 8.6048 - val_accuracy: 0.0629\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 41ms/step - loss: 5.1772 - accuracy: 0.0345 - val_loss: 8.8208 - val_accuracy: 0.0629\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 43ms/step - loss: 5.1779 - accuracy: 0.0345 - val_loss: 8.6880 - val_accuracy: 0.0629\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1767 - accuracy: 0.0345 - val_loss: 8.5294 - val_accuracy: 0.0629\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 44ms/step - loss: 5.1766 - accuracy: 0.0345 - val_loss: 8.5023 - val_accuracy: 0.0629\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 5.1753 - accuracy: 0.0345 - val_loss: 8.4455 - val_accuracy: 0.0629\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 1s 98ms/step - loss: 5.1774 - accuracy: 0.0345 - val_loss: 8.4628 - val_accuracy: 0.0629\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 1s 92ms/step - loss: 5.1791 - accuracy: 0.0345 - val_loss: 8.4482 - val_accuracy: 0.0629\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 1s 82ms/step - loss: 5.1780 - accuracy: 0.0345 - val_loss: 8.6015 - val_accuracy: 0.0629\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 1s 85ms/step - loss: 5.1798 - accuracy: 0.0345 - val_loss: 8.8232 - val_accuracy: 0.0629\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 1s 85ms/step - loss: 5.1780 - accuracy: 0.0345 - val_loss: 8.7597 - val_accuracy: 0.0629\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 49ms/step - loss: 5.1757 - accuracy: 0.0345 - val_loss: 8.6190 - val_accuracy: 0.0629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "2q9cLPiMjdf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_model1 = history1.history['val_loss'][-1]\n",
        "val_loss_model2 = history2.history['val_loss'][-1]\n",
        "\n",
        "print(f\"Validation Loss for Model 1: {val_loss_model1}\")\n",
        "print(f\"Validation Loss for Model 2: {val_loss_model2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEA_D-d0jfq-",
        "outputId": "cbd27219-3526-4e1d-a150-099e09d86842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss for Model 1: 10.128071784973145\n",
            "Validation Loss for Model 2: 8.619034767150879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if val_loss_model1 < val_loss_model2:\n",
        "    best_model = model1\n",
        "    best_model_name = \"best_model1.h5\"\n",
        "else:\n",
        "    best_model = model2\n",
        "    best_model_name = \"best_model2.h5\"\n",
        "\n",
        "best_model.save(best_model_name)\n",
        "print(f\"Saved the best model as {best_model_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttoEvdS3jnCc",
        "outputId": "a0cef1a7-f963-4005-a655-0dca72ff93d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved the best model as best_model2.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the previously saved model\n",
        "model = load_model('best_model2.h5')\n",
        "\n",
        "def predict_next_words(model, tokenizer, text, num_words=1):\n",
        "    \"\"\"\n",
        "    Predict the next set of words using the trained model.\n",
        "\n",
        "    Args:\n",
        "    - model (keras.Model): The trained model.\n",
        "    - tokenizer (Tokenizer): The tokenizer object used for preprocessing.\n",
        "    - text (str): The input text.\n",
        "    - num_words (int): The number of words to predict.\n",
        "\n",
        "    Returns:\n",
        "    - str: The predicted words.\n",
        "    \"\"\"\n",
        "    for _ in range(num_words):\n",
        "        # Tokenize and pad the text\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=5, padding='pre')\n",
        "\n",
        "        # Predict the next word\n",
        "        predicted_probs = model.predict(sequence, verbose=0)\n",
        "        predicted = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        # Convert the predicted word index to a word\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "\n",
        "        # Append the predicted word to the text\n",
        "        text += \" \" + output_word\n",
        "\n",
        "    return ' '.join(text.split(' ')[-num_words:])\n",
        "\n",
        "\n",
        "# Prompt the user for input\n",
        "user_input = input(\"Please type five words in Shona: \")\n",
        "\n",
        "# Predict the next words\n",
        "predicted_words = predict_next_words(model, tokenizer, user_input, num_words=3)\n",
        "print(f\"The next words might be: {predicted_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8IEGbiPj0hN",
        "outputId": "2f03e55a-1c24-47d2-c6e0-b8c5e2f972cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please type five words in Shona: sezvo chiri chinhu chinokosha kana\n",
            "The next words might be: kana kana kana\n"
          ]
        }
      ]
    }
  ]
}